# ğŸ›¡ï¸ Mini SOC Agentique â€” IA Locale (LM Studio / Ollama)

> SystÃ¨me de dÃ©tection et rÃ©ponse aux incidents de sÃ©curitÃ© automatisÃ©,  
> basÃ© sur 4 agents coopÃ©rants et une IA locale (Mistral 7B).

![Python](https://img.shields.io/badge/Python-3.10+-blue?style=flat-square&logo=python)
![Flask](https://img.shields.io/badge/Flask-3.x-black?style=flat-square&logo=flask)
![LM Studio](https://img.shields.io/badge/LM_Studio-compatible-purple?style=flat-square)
![Ollama](https://img.shields.io/badge/Ollama-compatible-orange?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

---

## ğŸ“‹ Description

Ce projet implÃ©mente un **Mini SOC (Security Operations Center) Agentique** composÃ© de 4 agents autonomes qui communiquent entre eux via HTTP REST en JSON pour automatiser la dÃ©tection, l'analyse et la rÃ©ponse aux incidents de sÃ©curitÃ©.

```
[SENSOR] â”€â”€â–º [COLLECTOR] â”€â”€â–º [ANALYZER (IA)] â”€â”€â–º [RESPONDER]
  Port:           6001              6002               6003
```

L'**ANALYZER** interroge un LLM local (via LM Studio ou Ollama) pour classifier chaque menace et recommander une action automatique.

---

## ğŸ—ï¸ Architecture

| Agent | Port | RÃ´le |
|-------|------|------|
| **SENSOR** | â€” | GÃ©nÃ¨re des Ã©vÃ©nements de sÃ©curitÃ© simulÃ©s (SSH brute force, port scan, etc.) |
| **COLLECTOR** | 6001 | Centralise et horodate les Ã©vÃ©nements, les transmet Ã  l'Analyzer |
| **ANALYZER** | 6002 | Analyse via LLM local, classifie la menace, recommande une action |
| **RESPONDER** | 6003 | Applique l'action : blocage IP, ticket d'incident, escalade |

### Communication
- **Format** : JSON
- **Protocole** : HTTP REST
- **Auth** : Token Bearer (`X-Auth-Token`)
- **LLM API** : OpenAI-compatible (`/v1/chat/completions`)

---

## ğŸ“ Structure du projet

```
mini-soc-agents/
â”‚
â”œâ”€â”€ sensor.py          # Agent SENSOR â€” gÃ©nÃ©ration d'Ã©vÃ©nements
â”œâ”€â”€ collector.py       # Agent COLLECTOR â€” centralisation (port 6001)
â”œâ”€â”€ analyzer.py        # Agent ANALYZER + IA locale (port 6002)
â”œâ”€â”€ responder.py       # Agent RESPONDER â€” actions (port 6003)
â”œâ”€â”€ start_all.sh       # Script de dÃ©marrage automatique
â””â”€â”€ README.md          # Ce fichier
```

---

## âš™ï¸ PrÃ©requis

- **OS** : Linux (Ubuntu 22.04+ / Kali Linux)
- **Python** : 3.10+
- **LLM** : LM Studio ou Ollama avec Mistral 7B Instruct

### DÃ©pendances Python
```bash
pip3 install flask requests --break-system-packages
```
ou
```bash
sudo apt install python3-flask python3-requests -y
```

---

## ğŸš€ Installation & DÃ©marrage

### Ã‰tape 1 â€” Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/VOTRE_USERNAME/mini-soc-agents.git
cd mini-soc-agents
```

### Ã‰tape 2 â€” Installer les dÃ©pendances
```bash
pip3 install flask requests --break-system-packages
```

### Ã‰tape 3 â€” DÃ©marrer le LLM local

**Option A â€” Ollama (recommandÃ© sur Linux) :**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral
ollama serve
```

**Option B â€” LM Studio :**
```bash
# TÃ©lÃ©charger l'AppImage depuis https://lmstudio.ai
chmod +x LM-Studio-*.AppImage
./LM-Studio-*.AppImage --no-sandbox
# Charger Mistral 7B Instruct et dÃ©marrer le serveur local (port 1234)
```

### Ã‰tape 4 â€” VÃ©rifier que le LLM rÃ©pond
```bash
# Ollama
curl http://127.0.0.1:11434/v1/models

# LM Studio
curl http://127.0.0.1:1234/v1/models
```

### Ã‰tape 5 â€” Lancer le Mini SOC
```bash
chmod +x start_all.sh
bash start_all.sh
```

---

## ğŸ”§ Configuration

Dans `analyzer.py`, modifier selon votre LLM :

```python
# Pour Ollama
LM_STUDIO_URL = "http://127.0.0.1:11434/v1/chat/completions"
# model = "mistral"

# Pour LM Studio
LM_STUDIO_URL = "http://127.0.0.1:1234/v1/chat/completions"
# model = "mistral-7b-instruct"
```

---

## ğŸ“Š Exemple de flux complet

```
[SENSOR]   ssh_failed depuis 192.168.56.102 â†’ HTTP 200
[COLLECTOR] ReÃ§u: ssh_failed | src: 192.168.56.102 | Total: 1
[COLLECTOR] â†’ ANALYZER: HTTP 200
[ANALYZER] Analyse: ssh_failed depuis 192.168.56.102
[ANALYZER] â†’ severity=Ã‰levÃ© | action=block_ip
[RESPONDER] *** status=blocked, ip=192.168.56.102 ***
```

### Types d'Ã©vÃ©nements simulÃ©s

| Type | SÃ©vÃ©ritÃ© | Action |
|------|----------|--------|
| `ssh_failed` | Ã‰levÃ© | `block_ip` |
| `port_scan` | Moyen | `create_ticket` |
| `http_anomaly` | Ã‰levÃ© | `block_ip` |
| `dns_query_suspicious` | Critique | `block_ip` |
| `malware_hash_detected` | Critique | `escalate` |
| `login_success_unusual` | Faible | `monitor` |

---

## ğŸ“¡ API Endpoints

### COLLECTOR â€” port 6001
```bash
POST /collect     # Recevoir un Ã©vÃ©nement
GET  /events      # Lister tous les Ã©vÃ©nements
GET  /health      # Statut du service
```

### ANALYZER â€” port 6002
```bash
POST /analyze     # Analyser un Ã©vÃ©nement via LLM
GET  /health      # Statut du service
```

### RESPONDER â€” port 6003
```bash
POST /respond     # Appliquer une action
GET  /status      # IPs bloquÃ©es + tickets crÃ©Ã©s
GET  /health      # Statut du service
```

### VÃ©rifier les rÃ©sultats aprÃ¨s exÃ©cution
```bash
# IPs bloquÃ©es et tickets crÃ©Ã©s
curl -s http://127.0.0.1:6003/status | python3 -m json.tool

# Tous les Ã©vÃ©nements collectÃ©s
curl -s http://127.0.0.1:6001/events | python3 -m json.tool
```

---

## ğŸ”’ SÃ©curitÃ© (simulation)

Tous les agents utilisent un token d'authentification dans les headers HTTP :
```
X-Auth-Token: soc-secret-token-2024
```
> âš ï¸ Token statique â€” Ã  remplacer par JWT ou mTLS en production.

---

## ğŸ“ˆ Limites & AmÃ©liorations possibles

### Limites actuelles
- Stockage en mÃ©moire (donnÃ©es perdues au redÃ©marrage)
- Token d'authentification statique
- Pas de corrÃ©lation d'Ã©vÃ©nements (chaque Ã©vÃ©nement analysÃ© indÃ©pendamment)
- Latence LLM (2â€“10 secondes par Ã©vÃ©nement)

### AmÃ©liorations suggÃ©rÃ©es
- [ ] Base de donnÃ©es (PostgreSQL / Elasticsearch)
- [ ] HTTPS + mTLS entre agents
- [ ] CorrÃ©lation d'Ã©vÃ©nements (dÃ©tection de patterns)
- [ ] Dashboard temps rÃ©el (Grafana / Kibana)
- [ ] IntÃ©gration Threat Intelligence (AbuseIPDB)
- [ ] Interface de supervision humaine (SOAR)
- [ ] File de messages (RabbitMQ / Kafka)

---

## ğŸ§ª TP â€” Contexte pÃ©dagogique

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre d'un **TP â€” Mini SOC Agentique avec IA Locale (LM Studio)** avec les objectifs suivants :
- Comprendre l'architecture d'un SOC rÃ©el
- DÃ©couvrir les agents logiciels coopÃ©rants
- IntÃ©grer une IA locale pour l'analyse automatique
- Observer le flux complet : dÃ©tection â†’ analyse â†’ dÃ©cision â†’ action

**Environnement :**
- VM Ubuntu / Kali Linux (serveur SOC)
- LM Studio ou Ollama avec Mistral 7B Instruct
- Python 3 + Flask + requests

---

## ğŸ‘¨â€ğŸ’» Auteur

**Nom :** *Taha El Yacoubi* & *Azzedine lazrarqi*  
**Module :** Cryptography  
**Niveau :** 5th year engineering  
**AnnÃ©e :** 2025â€“2026

---

